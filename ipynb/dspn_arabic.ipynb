{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"../../\")\n",
    "import libspn as spn\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_decomps = 2\n",
    "num_subsets = 2\n",
    "num_input_mixtures = 2\n",
    "num_roots = 1\n",
    "num_mixtures = 2\n",
    "input_dist = spn.DenseSPNGeneratorLayerNodes.InputDist.RAW\n",
    "\n",
    "# Additive smoothing during learning\n",
    "additive_smoothing=10\n",
    "min_additive_smoothing=1\n",
    "smoothing_decay=0.2\n",
    "\n",
    "# Weight initialization\n",
    "# weight_init_value = 1\n",
    "weight_init_value = spn.ValueType.RANDOM_UNIFORM(1, 2)\n",
    "\n",
    "# Type of inference during upward pass of learning\n",
    "value_inference_type = spn.InferenceType.MARGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_data = []\n",
    "male_labels = np.concatenate([np.ones(330) * i for i in range(10)])\n",
    "female_data = []\n",
    "female_labels = np.concatenate([np.ones(330) * i for i in range(10)])\n",
    "with open('/home/jos/datasets/arabic/Train_Arabic_Digit.txt') as f:\n",
    "    count = 0\n",
    "    block = []\n",
    "    for line in f.readlines():\n",
    "        words = line.split(' ')\n",
    "        if len(words) == 13 and all(words):\n",
    "            block.append([float(w) for w in words])\n",
    "        else:\n",
    "            if block:\n",
    "                if (count // 330) % 2 == 0:\n",
    "                    male_data.append(block)\n",
    "                else:\n",
    "                    female_data.append(block)\n",
    "            count += 1\n",
    "            block = []\n",
    "    if block:\n",
    "        female_data.append(block)\n",
    "            \n",
    "# import pandas as pd\n",
    "\n",
    "# train_data = pd.read_csv('/home/jos/datasets/arabic/pendigits.tra', delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = male_data + female_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_lens(sequences):\n",
    "    return [len(seq) for seq in sequences]\n",
    "\n",
    "def pad_sequences(sequences, sequence_maxlen, padding_val=-1):\n",
    "    ret = []\n",
    "    for seq in sequences:\n",
    "        arr = np.asarray(seq)\n",
    "        ret.append(np.concatenate([\n",
    "            padding_val * np.ones((sequence_maxlen - len(seq),) + arr.shape[1:]),\n",
    "            arr\n",
    "        ]))\n",
    "    return np.asarray(ret)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263256, 13)\n"
     ]
    }
   ],
   "source": [
    "data_stacked = np.concatenate(all_data, axis=0)\n",
    "print(data_stacked.shape)\n",
    "padded = pad_sequences(all_data[:2], 93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "dense_gen = spn.DenseSPNGeneratorLayerNodes(num_decomps=num_decomps, num_subsets=num_subsets, num_mixtures=num_mixtures, \n",
    "                                            input_dist=input_dist, num_input_mixtures=num_input_mixtures,\n",
    "                                            node_type=spn.DenseSPNGeneratorLayerNodes.NodeType.LAYER)\n",
    "\n",
    "contvars = spn.GaussianLeaf(\n",
    "    max_steps=93, num_vars=13, num_components=2, data=data_stacked, dynamic=True, use_prior=True,\n",
    "    learn_dist_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = dense_gen.generate(contvars)\n",
    "template_heads = root.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(template_heads[0].node._num_or_size_prods)\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "dense_gen = spn.DenseSPNGeneratorLayerNodes(num_decomps=num_decomps, num_subsets=num_subsets, num_mixtures=num_mixtures, \n",
    "                                            input_dist=input_dist, num_input_mixtures=num_input_mixtures, \n",
    "                                            node_type=spn.DenseSPNGeneratorLayerNodes.NodeType.LAYER)\n",
    "\n",
    "contvars = spn.GaussianLeaf(\n",
    "    max_steps=93, num_vars=13, num_components=2, data=data_stacked, dynamic=True, use_prior=True,\n",
    "    learn_dist_params=True, total_counts_init=100)\n",
    "\n",
    "top_per_digit = []\n",
    "for d in range(10):\n",
    "    root = dense_gen.generate(contvars)\n",
    "#     print(root.values[0].node)\n",
    "    template_head = root.values[0].node\n",
    "    \n",
    "    interface = spn.DynamicInterface(name=\"Interface\")\n",
    "    interface.set_source(template_head)\n",
    "    \n",
    "    interface_mixtures = spn.ParSums(interface, num_sums=template_head.num_prods, name=\"InterfaceMixtures\", interface_head=True)\n",
    "    \n",
    "    prod_sizes = template_head.num_or_size_prods\n",
    "    offset = 0\n",
    "    new_inputs, new_prod_sizes = [], []\n",
    "    for i, size in enumerate(prod_sizes):\n",
    "        new_inputs.extend(template_head.inputs[offset:offset+size])\n",
    "        new_inputs.append(spn.Input(node=interface_mixtures, indices=i))\n",
    "        new_prod_sizes.append(size + 1)\n",
    "        offset += size\n",
    "    template_head.set_values(*new_inputs)\n",
    "    template_head.set_prod_sizes(new_prod_sizes)\n",
    "    \n",
    "    top_per_digit.append(root)\n",
    "    #     top_per_digit.append(spn.Sum(*template_heads, name=\"Top{}\".format(d)))\n",
    "    \n",
    "root = spn.Sum(*top_per_digit)\n",
    "spn.generate_weights(root, init_value=weight_init_value)\n",
    "latent = root.generate_ivs()\n",
    "\n",
    "print(root.get_num_nodes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len_ph = tf.placeholder(tf.int32, [None])\n",
    "additive_smoothing_var = tf.Variable(additive_smoothing, dtype=spn.conf.dtype)\n",
    "learning = spn.EMLearning(root, log=True, value_inference_type = value_inference_type,\n",
    "                          additive_smoothing=additive_smoothing_var, sequence_lens=sequence_len_ph,\n",
    "                          initial_accum_value=100)\n",
    "init_weights = spn.initialize_weights(root)\n",
    "reset_accumulators = learning.reset_accumulators()\n",
    "accumulate_updates = learning.accumulate_updates()\n",
    "update_spn = learning.update_spn()\n",
    "train_likelihood = learning.likelihood()\n",
    "avg_train_likelihood = tf.reduce_mean(train_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lens = sequence_lens(all_data)\n",
    "train_set = pad_sequences(all_data, 93).transpose((1, 0, 2))\n",
    "train_labels = np.expand_dims(np.concatenate([male_labels, female_labels]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6600,) (93, 6600, 13) (6600, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(seq_lens).shape, train_set.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = None\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531d05c5dbab4b6eac3d87037d5a3245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=64), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Avg likelihood: -737.9169101715088\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb67d84676ad47de9da7b734de95d3f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=64), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if sess is not None:\n",
    "    sess.close()\n",
    "sess = tf.Session()\n",
    "    \n",
    "sess.run(init_weights)\n",
    "sess.run(reset_accumulators)\n",
    "\n",
    "num_batches=64\n",
    "batch_size = train_set.shape[1] // num_batches\n",
    "prev_likelihood = 100\n",
    "likelihood = 0\n",
    "for epoch in range(num_epochs):\n",
    "    prev_likelihood=likelihood\n",
    "    likelihoods = []\n",
    "    pbar = tqdm_notebook(range(num_batches))\n",
    "    train_set, train_labels = shuffle(train_set.transpose((1, 0, 2)), train_labels)\n",
    "    train_set = train_set.transpose((1, 0, 2))\n",
    "    for batch in pbar:\n",
    "        start = (batch)*batch_size\n",
    "        stop = (batch+1)*batch_size\n",
    "        # Adjust smoothing\n",
    "        ads=max(np.exp(-epoch*smoothing_decay)*additive_smoothing, min_additive_smoothing)\n",
    "        sess.run(additive_smoothing_var.assign(ads))\n",
    "        # Run accumulate_updates\n",
    "        train_likelihoods_arr, avg_train_likelihood_val, _, = \\\n",
    "                sess.run([train_likelihood, avg_train_likelihood, accumulate_updates],\n",
    "                        feed_dict={contvars: train_set[:, start:stop],\n",
    "                                   latent: train_labels[start:stop],\n",
    "                                   sequence_len_ph: seq_lens[start:stop]})\n",
    "        # Print avg likelihood of this batch data on previous batch weights\n",
    "        likelihoods.append(avg_train_likelihood_val)\n",
    "        pbar.set_description(\"lh: {}\".format(avg_train_likelihood_val))\n",
    "        # Update weights\n",
    "        sess.run(update_spn)\n",
    "    likelihood = sum(likelihoods) / len(likelihoods)\n",
    "    print(\"Avg likelihood: %s\" % (likelihood))\n",
    "    sess.run(reset_accumulators)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
