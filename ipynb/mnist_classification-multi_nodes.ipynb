{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import libspn as spn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as scp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type of input distributions for dense generator\n",
    "input_dist = spn.DenseSPNGeneratorMultiNodes.InputDist.RAW\n",
    "\n",
    "num_decomps=1\n",
    "num_subsets=5\n",
    "num_mixtures=2\n",
    "\n",
    "# Number of input mixtures for dense generator\n",
    "num_input_mixtures = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Additive smoothing during learning\n",
    "additive_smoothing=100\n",
    "min_additive_smoothing=1\n",
    "smoothing_decay=0.2\n",
    "\n",
    "# Weight initialization\n",
    "# weight_init_value = 1\n",
    "weight_init_value = spn.ValueType.RANDOM_UNIFORM(10, 11)\n",
    "\n",
    "# Type of inference during upward pass of learning\n",
    "value_inference_type = spn.InferenceType.MARGINAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = tf.contrib.learn.datasets.mnist.read_data_sets(\"mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_mnist(img):\n",
    "    img=np.reshape(img, (14,14))\n",
    "    plt.imshow(img, interpolation='none')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_set(data):\n",
    "    threshold = 20\n",
    "    images = np.reshape(data, (-1, 28,28))\n",
    "    resized=[]\n",
    "    for i in range(images.shape[0]):\n",
    "        resized.append((scp.misc.imresize(images[i, :, :], 0.5, interp='nearest').ravel() > threshold).astype(dtype=int))\n",
    "    images=np.vstack(resized)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images = process_set(datasets.train.images)\n",
    "test_images = process_set(datasets.test.images)\n",
    "train_labels = datasets.train.labels\n",
    "test_labels = datasets.test.labels\n",
    "validation_labels = datasets.validation.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_mnist(train_images[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_images_0 = train_images[train_labels==0]\n",
    "train_images_1 = train_images[train_labels==1]\n",
    "test_images_0 = test_images[test_labels==0]\n",
    "test_images_1 = test_images[test_labels==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images_0.shape)\n",
    "show_mnist(train_images_0[334])\n",
    "print(train_images_1.shape)\n",
    "show_mnist(train_images_1[22])\n",
    "print(test_images_0.shape)\n",
    "show_mnist(test_images_0[334])\n",
    "print(test_images_1.shape)\n",
    "show_mnist(test_images_1[22])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate SPN structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivs = spn.IVs(num_vars=196, num_vals=2)\n",
    "dense_gen = spn.DenseSPNGeneratorMultiNodes(num_decomps=num_decomps, num_subsets=num_subsets,\n",
    "                                            num_mixtures=num_mixtures, input_dist=input_dist,\n",
    "                                            num_input_mixtures=num_input_mixtures, multi_nodes=True)\n",
    "root0 = dense_gen.generate(ivs)\n",
    "root1 = dense_gen.generate(ivs)\n",
    "root = spn.Sum(root0, root1)\n",
    "spn.generate_weights(root, init_value=weight_init_value)\n",
    "latent = root.generate_ivs()\n",
    "print(root.get_num_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Learning Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "additive_smoothing_var = tf.Variable(additive_smoothing, dtype=spn.conf.dtype)\n",
    "learning = spn.EMLearning(root, log=True, value_inference_type = value_inference_type,\n",
    "                          additive_smoothing=additive_smoothing_var)\n",
    "init_weights = spn.initialize_weights(root)\n",
    "reset_accumulators = learning.reset_accumulators()\n",
    "accumulate_updates = learning.accumulate_updates()\n",
    "update_spn = learning.update_spn()\n",
    "train_likelihood=learning.value.values[root]\n",
    "avg_train_likelihood = tf.reduce_mean(train_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.concatenate([train_images_0, train_images_1], 0)\n",
    "train_labels = np.concatenate([np.ones((train_images_0.shape[0]))*0, \n",
    "                               np.ones((train_images_1.shape[0]))*1])\n",
    "train_labels=np.reshape(train_labels, (-1, 1))\n",
    "\n",
    "if sess is not None:\n",
    "    sess.close()\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init_weights)\n",
    "sess.run(reset_accumulators)\n",
    "\n",
    "num_batches=1\n",
    "batch_size = train_set.shape[0] // num_batches\n",
    "prev_likelihood = 100\n",
    "likelihood = 0\n",
    "epoch = 0\n",
    "while abs(prev_likelihood - likelihood)>0.1:\n",
    "    prev_likelihood=likelihood\n",
    "    likelihoods = []\n",
    "    for batch in range(num_batches):\n",
    "        start = (batch)*batch_size\n",
    "        stop = (batch+1)*batch_size\n",
    "        print(\"EPOCH\", epoch, \"BATCH\", batch, \"SAMPLES\", start, stop)\n",
    "        # Adjust smoothing\n",
    "        ads=max(np.exp(-epoch*smoothing_decay)*additive_smoothing, min_additive_smoothing)\n",
    "        sess.run(additive_smoothing_var.assign(ads))\n",
    "        print(\"Smoothing: \", sess.run(additive_smoothing_var)) \n",
    "        # Run accumulate_updates\n",
    "        train_likelihoods_arr, avg_train_likelihood_val, _, = \\\n",
    "                sess.run([train_likelihood, avg_train_likelihood, accumulate_updates],\n",
    "                        feed_dict={ivs: train_set[start:stop],\n",
    "                                   latent: train_labels[start:stop]})\n",
    "        # Print avg likelihood of this batch data on previous batch weights\n",
    "        print(\"Avg likelihood (this batch data on previous weights): %s\" % (avg_train_likelihood_val))\n",
    "        likelihoods.append(avg_train_likelihood_val)\n",
    "        # Update weights\n",
    "        sess.run(update_spn)\n",
    "    likelihood = sum(likelihoods) / len(likelihoods)\n",
    "    print(\"Avg likelihood: %s\" % (likelihood))\n",
    "    epoch+=1\n",
    "    sess.run(reset_accumulators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Testing Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpe_state_gen = spn.MPEState(log=True, value_inference_type=spn.InferenceType.MPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpe_ivs, mpe_latent = mpe_state_gen.get_state(root, ivs, latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPE of the whole network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpe_ivs_val, mpe_latent_val = sess.run([mpe_ivs, mpe_latent], feed_dict={ivs: np.ones((1, 14*14), dtype=int) * -1,\n",
    "                                                                       latent: [[-1]]})\n",
    "show_mnist(mpe_ivs_val)\n",
    "print(mpe_latent_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPE for 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpe_ivs_val, mpe_latent_val = sess.run([mpe_ivs, mpe_latent], feed_dict={ivs: np.ones((1, 14*14), dtype=int) * -1,\n",
    "                                                                       latent: [[0]]})\n",
    "show_mnist(mpe_ivs_val)\n",
    "print(mpe_latent_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPE for 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpe_ivs_val, mpe_latent_val = sess.run([mpe_ivs, mpe_latent], feed_dict={ivs: np.ones((1, 14*14), dtype=int) * -1,\n",
    "                                                                       latent: [[1]]})\n",
    "show_mnist(mpe_ivs_val)\n",
    "print(mpe_latent_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpe_latent_val = sess.run([mpe_latent], feed_dict={ivs: train_set, \n",
    "                                                   latent: np.ones((train_set.shape[0], 1))*-1})\n",
    "result=(mpe_latent_val==train_labels)\n",
    "np.sum(result) / train_labels.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classification of Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = np.concatenate([test_images_0, test_images_1], 0)\n",
    "test_labels = np.concatenate([np.ones((test_images_0.shape[0]))*0, \n",
    "                               np.ones((test_images_1.shape[0]))*1])\n",
    "test_labels=np.reshape(test_labels, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpe_latent_val = sess.run([mpe_latent], feed_dict={ivs: test_set, \n",
    "                                                   latent: np.ones((test_set.shape[0], 1))*-1})\n",
    "result=(mpe_latent_val==test_labels)\n",
    "np.sum(result) / test_labels.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
