{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import libspn as spn\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_decomps = 5\n",
    "num_subsets = 5\n",
    "num_input_mixtures = 5\n",
    "num_roots = 4\n",
    "num_mixtures = 5\n",
    "input_dist = spn.DenseSPNGenerator.InputDist.MIXTURE\n",
    "\n",
    "# Additive smoothing during learning\n",
    "additive_smoothing=10\n",
    "min_additive_smoothing=1\n",
    "smoothing_decay=0.2\n",
    "\n",
    "# Weight initialization\n",
    "# weight_init_value = 1\n",
    "weight_init_value = spn.ValueType.RANDOM_UNIFORM(1, 2)\n",
    "\n",
    "# Type of inference during upward pass of learning\n",
    "value_inference_type = spn.InferenceType.MARGINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "980734\n",
      "32\n",
      "16\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "with open('/home/jos/datasets/msbnc/msnbc990928.seq', 'r') as f:\n",
    "    line = f.readline()\n",
    "    while 'Sequences:' not in line:\n",
    "        line = f.readline()\n",
    "    f.readline()  # also skip blank line\n",
    "    \n",
    "    for line in f.readlines():\n",
    "        seq = [int(a) - 1 for a in line.strip().split(' ')]\n",
    "        if len(seq) <= 32:\n",
    "            data.append([int(a) - 1 for a in line.strip().split(' ')])\n",
    "\n",
    "print(len(data))\n",
    "print(max([len(s) for s in data]))\n",
    "print(max([max(s) for s in data]))\n",
    "print(min([min(s) for s in data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_lens(sequences):\n",
    "    return [len(seq) for seq in sequences]\n",
    "\n",
    "def pad_sequences(sequences, sequence_maxlen):\n",
    "    ret = []\n",
    "    for seq in sequences:\n",
    "        arr = np.asarray(seq)\n",
    "        ret.append(np.concatenate([\n",
    "            -1 * np.ones((sequence_maxlen - len(seq),) + arr.shape[1:]),\n",
    "            arr\n",
    "        ]))\n",
    "    return np.asarray(ret)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "dense_gen = spn.DenseSPNGenerator(num_decomps=num_decomps, num_subsets=num_subsets, num_mixtures=num_mixtures, \n",
    "                                  input_dist=input_dist, \n",
    "                                  num_input_mixtures=num_input_mixtures)\n",
    "\n",
    "ivs = spn.DynamicIVs(max_steps=32, num_vars=1, num_vals=17)\n",
    "\n",
    "top_per_digit = []\n",
    "# for d in range(10):\n",
    "template_heads = dense_gen.generate(\n",
    "    ivs, num_roots=num_roots, root_sum=False, root_name=\"DenseDigit\")\n",
    "\n",
    "interfaces = []\n",
    "for i, head in enumerate(template_heads):\n",
    "    intf = spn.DynamicInterface(name=\"Interface\")\n",
    "    intf.set_source(head)\n",
    "    interfaces.append(intf)\n",
    "\n",
    "interface_mixtures = []\n",
    "for i, head in enumerate(template_heads):\n",
    "    mixture = spn.Sum(*interfaces, name=\"InterfaceMixture\")\n",
    "    head.set_values(mixture, *head.values)\n",
    "\n",
    "root = spn.Sum(*template_heads, name=\"Top\")\n",
    "spn.generate_weights(root, init_value=weight_init_value)\n",
    "# latent = root.generate_ivs()\n",
    "\n",
    "print(root.get_num_nodes())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jos/spn/libspn/libspn/utils/math.py:277: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] [tensorflow:warning] From /home/jos/spn/libspn/libspn/utils/math.py:277: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jos/spn/libspn/libspn/utils/math.py:281: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] [tensorflow:warning] From /home/jos/spn/libspn/libspn/utils/math.py:281: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jos/spn/libspn/libspn/graph/sum.py:299: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] [tensorflow:warning] From /home/jos/spn/libspn/libspn/graph/sum.py:299: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n"
     ]
    }
   ],
   "source": [
    "sequence_len_ph = tf.placeholder(tf.int32, [None])\n",
    "additive_smoothing_var = tf.Variable(additive_smoothing, dtype=spn.conf.dtype)\n",
    "learning = spn.EMLearning(root, log=True, value_inference_type = value_inference_type,\n",
    "                          additive_smoothing=additive_smoothing_var, sequence_lens=sequence_len_ph)\n",
    "init_weights = spn.initialize_weights(root)\n",
    "reset_accumulators = learning.reset_accumulators()\n",
    "accumulate_updates = learning.accumulate_updates()\n",
    "update_spn = learning.update_spn()\n",
    "train_likelihood = learning.likelihood()\n",
    "avg_train_likelihood = tf.reduce_mean(train_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.expand_dims(pad_sequences(data, 32), -1).transpose((1, 0, 2))\n",
    "seq_lens = sequence_lens(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b4949de8734caa8ef00f80abf7c33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=84), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Avg likelihood: -53.388345173427034\n",
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133302761917482190ded03466d06bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=84), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg likelihood: -53.3055636542184\n",
      "Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b2a10036314e0682dbc762c2e140f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=84), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avg likelihood: -53.30555734180269\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if sess is not None:\n",
    "    sess.close()\n",
    "sess = tf.Session()\n",
    "    \n",
    "sess.run(init_weights)\n",
    "sess.run(reset_accumulators)\n",
    "\n",
    "num_batches=84\n",
    "batch_size = train_set.shape[1] // num_batches\n",
    "prev_likelihood = 100\n",
    "likelihood = 0\n",
    "epoch = 0\n",
    "while abs(prev_likelihood - likelihood)>0.025:\n",
    "    prev_likelihood=likelihood\n",
    "    likelihoods = []\n",
    "    print(\"Epoch\", epoch)\n",
    "    pbar = tqdm_notebook(range(num_batches))\n",
    "    for batch in pbar:\n",
    "        start = (batch)*batch_size\n",
    "        stop = (batch+1)*batch_size\n",
    "#         print(\"EPOCH\", epoch, \"BATCH\", batch, \"SAMPLES\", start, stop)\n",
    "        # Adjust smoothing\n",
    "        ads=max(np.exp(-epoch*smoothing_decay)*additive_smoothing, min_additive_smoothing)\n",
    "        sess.run(additive_smoothing_var.assign(ads))\n",
    "#         print(\"Smoothing: \", sess.run(additive_smoothing_var)) \n",
    "        # Run accumulate_updates\n",
    "        train_likelihoods_arr, avg_train_likelihood_val, _, = \\\n",
    "                sess.run([train_likelihood, avg_train_likelihood, accumulate_updates],\n",
    "                        feed_dict={ivs: train_set[:, start:stop],\n",
    "                                   sequence_len_ph: seq_lens[start:stop]})\n",
    "        # Print avg likelihood of this batch data on previous batch weights\n",
    "        likelihoods.append(avg_train_likelihood_val)\n",
    "        sess.run(update_spn)\n",
    "        \n",
    "        pbar.set_description(\"Likelihood: %s\" % (avg_train_likelihood_val))\n",
    "    likelihood = sum(likelihoods) / len(likelihoods)\n",
    "    print(\"Avg likelihood: %s\" % (likelihood))\n",
    "    epoch+=1\n",
    "    sess.run(reset_accumulators)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
