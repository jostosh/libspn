{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow/core/kernels/reduction_ops_max.cc\n",
    "```cpp\n",
    "#include \"tensorflow/core/kernels/reduction_ops_common.h\"\n",
    "\n",
    "namespace tensorflow {\n",
    "\n",
    "#define REGISTER_CPU_KERNELS(type)        \\\n",
    "  REGISTER_KERNEL_BUILDER(                \\\n",
    "      Name(\"Max\")                         \\\n",
    "          .Device(DEVICE_CPU)             \\\n",
    "          .TypeConstraint<type>(\"T\")      \\\n",
    "          .TypeConstraint<int32>(\"Tidx\"), \\\n",
    "      ReductionOp<CPUDevice, type, Eigen::internal::MaxReducer<type>>);\n",
    "TF_CALL_REAL_NUMBER_TYPES(REGISTER_CPU_KERNELS);\n",
    "#undef REGISTER_CPU_KERNELS\n",
    "\n",
    "#if GOOGLE_CUDA\n",
    "\n",
    "#define REGISTER_GPU_KERNELS(type)          \\\n",
    "  REGISTER_KERNEL_BUILDER(                  \\\n",
    "      Name(\"Max\")                           \\\n",
    "          .Device(DEVICE_GPU)               \\\n",
    "          .TypeConstraint<type>(\"T\")        \\\n",
    "          .TypeConstraint<int32>(\"Tidx\")    \\\n",
    "          .HostMemory(\"reduction_indices\"), \\\n",
    "      ReductionOp<GPUDevice, type, Eigen::internal::MaxReducer<type>>);\n",
    "REGISTER_GPU_KERNELS(float);\n",
    "REGISTER_GPU_KERNELS(double);\n",
    "\n",
    "// A special GPU kernel for int32.\n",
    "// TODO(b/25387198): Also enable int32 in device memory. This kernel\n",
    "// registration requires all int32 inputs and outputs to be in host memory.\n",
    "REGISTER_KERNEL_BUILDER(\n",
    "    Name(\"Max\")\n",
    "        .Device(DEVICE_GPU)\n",
    "        .HostMemory(\"reduction_indices\")\n",
    "        .HostMemory(\"input\")\n",
    "        .HostMemory(\"output\")\n",
    "        .TypeConstraint<int32>(\"T\")\n",
    "        .TypeConstraint<int32>(\"Tidx\"),\n",
    "    ReductionOp<CPUDevice, int32, Eigen::internal::MaxReducer<int32>>);\n",
    "\n",
    "#undef REGISTER_GPU_KERNELS\n",
    "\n",
    "#endif\n",
    "\n",
    "#ifdef TENSORFLOW_USE_SYCL\n",
    "#define REGISTER_SYCL_KERNELS(type)         \\\n",
    "  REGISTER_KERNEL_BUILDER(                  \\\n",
    "      Name(\"Max\")                           \\\n",
    "          .Device(DEVICE_SYCL)              \\\n",
    "          .TypeConstraint<type>(\"T\")        \\\n",
    "          .TypeConstraint<int32>(\"Tidx\")    \\\n",
    "          .HostMemory(\"reduction_indices\"), \\\n",
    "      ReductionOp<SYCLDevice, type, Eigen::internal::MaxReducer<type>>);\n",
    "REGISTER_SYCL_KERNELS(float);\n",
    "REGISTER_SYCL_KERNELS(double);\n",
    "\n",
    "REGISTER_KERNEL_BUILDER(\n",
    "    Name(\"Max\")\n",
    "        .Device(DEVICE_SYCL)\n",
    "        .HostMemory(\"reduction_indices\")\n",
    "        .HostMemory(\"input\")\n",
    "        .HostMemory(\"output\")\n",
    "        .TypeConstraint<int32>(\"T\")\n",
    "        .TypeConstraint<int32>(\"Tidx\"),\n",
    "    ReductionOp<CPUDevice, int32, Eigen::internal::MaxReducer<int32>>);\n",
    "#undef REGISTER_SYCL_KERNELS\n",
    "#endif // TENSORFLOW_USE_SYCL\n",
    "\n",
    "}  // namespace tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow/tensorflow/core/kernels/reduction_ops.h\n",
    "```cpp\n",
    "\n",
    "#ifndef TENSORFLOW_KERNELS_REDUCTION_OPS_H_\n",
    "#define TENSORFLOW_KERNELS_REDUCTION_OPS_H_\n",
    "\n",
    "// Functor definitions for Reduction ops, must be compilable by nvcc.\n",
    "\n",
    "#include <iostream>\n",
    "#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n",
    "#include \"tensorflow/core/framework/op_kernel.h\"\n",
    "#include \"tensorflow/core/framework/tensor_types.h\"\n",
    "\n",
    "namespace tensorflow {\n",
    "namespace functor {\n",
    "\n",
    "template <typename Device, typename OUT_T, typename IN_T,\n",
    "          typename ReductionAxes, typename Reducer>\n",
    "void ReduceEigenImpl(const Device& d, OUT_T out, IN_T in,\n",
    "                     const ReductionAxes& reduction_axes,\n",
    "                     const Reducer& reducer) {\n",
    "  out.device(d) = in.reduce(reduction_axes, reducer);\n",
    "}\n",
    "\n",
    "// For most reducers, the identity is Reducer::initialize()\n",
    "template <typename Reducer>\n",
    "struct Identity {\n",
    "  static auto identity(const Reducer& reducer)\n",
    "      -> decltype(reducer.initialize()) {\n",
    "    return reducer.initialize();\n",
    "  }\n",
    "};\n",
    "\n",
    "// MeanReducer is a special case, since it doesn't technically have an identity.\n",
    "// Thus, ideally we'd return nan.  However, mean is instantiated for integer\n",
    "// types as well, so we do the nan override only for floating point types.\n",
    "#define FIX_MEAN_IDENTITY(T)                                    \\\n",
    "  template <>                                                   \\\n",
    "  struct Identity<Eigen::internal::MeanReducer<T>> {            \\\n",
    "    static T identity(const Eigen::internal::MeanReducer<T>&) { \\\n",
    "      return Eigen::NumTraits<T>::quiet_NaN();                  \\\n",
    "    }                                                           \\\n",
    "  };\n",
    "FIX_MEAN_IDENTITY(Eigen::half)\n",
    "FIX_MEAN_IDENTITY(float)\n",
    "FIX_MEAN_IDENTITY(double)\n",
    "FIX_MEAN_IDENTITY(complex64)\n",
    "FIX_MEAN_IDENTITY(complex128)\n",
    "#undef FIX_MEAN_IDENTITY\n",
    "\n",
    "template <typename Device, typename OUT_T, typename Reducer>\n",
    "void FillIdentityEigenImpl(const Device& d, OUT_T out, const Reducer& reducer) {\n",
    "  out.device(d) = out.constant(Identity<Reducer>::identity(reducer));\n",
    "}\n",
    "\n",
    "template <typename Device, typename Reducer>\n",
    "struct ReduceFunctor {\n",
    "  template <typename OUT_T, typename IN_T, typename ReductionAxes>\n",
    "  static void Reduce(OpKernelContext* ctx, OUT_T out, IN_T in,\n",
    "                     const ReductionAxes& reduction_axes,\n",
    "                     const Reducer& reducer);\n",
    "\n",
    "  template <typename OUT_T>\n",
    "  static void FillIdentity(const Device& d, OUT_T out, const Reducer& reducer);\n",
    "};\n",
    "\n",
    "}  // namespace functor\n",
    "}  // namespace tensorflow\n",
    "\n",
    "#endif  // TENSORFLOW_KERNELS_REDUCTION_OPS_H_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow/tensorflow/core/kernels/reduction_ops_common.cc\n",
    "```cpp\n",
    "\n",
    "#include \"tensorflow/core/kernels/reduction_ops_common.h\"\n",
    "\n",
    "#include \"tensorflow/core/lib/strings/str_util.h\"\n",
    "\n",
    "namespace tensorflow {\n",
    "\n",
    "TensorShape ReductionHelper::out_reshape() const {\n",
    "  TensorShape shape;\n",
    "  for (auto size : out_reshape_) shape.AddDim(size);\n",
    "  return shape;\n",
    "}\n",
    "\n",
    "// The final output shape must be allocated with this shape.\n",
    "TensorShape ReductionHelper::out_shape() const {\n",
    "  TensorShape shape;\n",
    "  for (auto size : out_shape_) shape.AddDim(size);\n",
    "  return shape;\n",
    "}\n",
    "\n",
    "TensorShape ReductionHelper::shuffled_shape() {\n",
    "  const int dims = data_reshape_.size();\n",
    "  TensorShape shape;\n",
    "  for (int i = reduce_first_axis_; i < dims; i += 2) {\n",
    "    shape.AddDim(data_reshape_[i]);\n",
    "  }\n",
    "  for (int i = !reduce_first_axis_; i < dims; i += 2) {\n",
    "    shape.AddDim(data_reshape_[i]);\n",
    "  }\n",
    "  return shape;\n",
    "}\n",
    "\n",
    "gtl::InlinedVector<int32, 8> ReductionHelper::permutation() {\n",
    "  const int dims = data_reshape_.size();\n",
    "  const int unreduced_dims = (dims + !reduce_first_axis_) / 2;\n",
    "  gtl::InlinedVector<int32, 8> perm(dims);\n",
    "  for (int i = 0; i < unreduced_dims; i++) {\n",
    "    perm[i] = 2 * i + reduce_first_axis_;\n",
    "  }\n",
    "  for (int i = unreduced_dims; i < dims; i++) {\n",
    "    perm[i] = 2 * (i - unreduced_dims) + !reduce_first_axis_;\n",
    "  }\n",
    "  return perm;\n",
    "}\n",
    "\n",
    "template <typename Tperm>\n",
    "Status SimplifyHelper(const Tensor& data, const Tensor& axis,\n",
    "                      gtl::InlinedVector<bool, 4>& bitmap) {\n",
    "  auto axis_vec = axis.flat<Tperm>();\n",
    "  for (int64 i = 0; i < axis.NumElements(); ++i) {\n",
    "    Tperm index = axis_vec(i);\n",
    "    if (index < -data.dims() || index >= data.dims()) {\n",
    "      return errors::InvalidArgument(\"Invalid reduction dimension (\", index,\n",
    "                                     \" for input with \", data.dims(),\n",
    "                                     \" dimension(s)\");\n",
    "    }\n",
    "    index = (index + data.dims()) % data.dims();\n",
    "    bitmap[index] = true;\n",
    "  }\n",
    "  return Status::OK();\n",
    "}\n",
    "\n",
    "Status ReductionHelper::Simplify(const Tensor& data, const Tensor& axis,\n",
    "                                 const bool keep_dims) {\n",
    "  // bitmap[i] indicates whether to reduce data along i-th axis.\n",
    "  gtl::InlinedVector<bool, 4> bitmap(data.dims(), false);\n",
    "  if (axis.dtype() == DT_INT32) {\n",
    "    TF_RETURN_IF_ERROR(SimplifyHelper<int32>(data, axis, bitmap));\n",
    "  } else {\n",
    "    TF_RETURN_IF_ERROR(SimplifyHelper<int64>(data, axis, bitmap));\n",
    "  }\n",
    "  // Output tensor's dim sizes.\n",
    "  out_shape_.clear();\n",
    "  for (int i = 0; i < data.dims(); ++i) {\n",
    "    if (!bitmap[i]) {\n",
    "      // If we are not reducing along dimension i.\n",
    "      out_shape_.push_back(data.dim_size(i));\n",
    "    } else if (keep_dims) {\n",
    "      // We are reducing along dimension i, but we want to keep the\n",
    "      // same number of dimensions, so we set the dimension of i to\n",
    "      // '1'.\n",
    "      out_shape_.push_back(1);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Depending on bitmap[i] and bitmap[i-1], we can collapse axis of\n",
    "  // the input data before doing the reduction on the resulting\n",
    "  // tensor.  The shape of the reduction is a reshape of the final\n",
    "  // output.\n",
    "\n",
    "  // We'll skip the leading 1s.\n",
    "  int dim_index = 0;\n",
    "  for (; dim_index < data.dims(); ++dim_index) {\n",
    "    if (data.dim_size(dim_index) != 1) break;\n",
    "  }\n",
    "  if (dim_index >= data.dims()) {\n",
    "    // Special case. The input is essentially a scalar.\n",
    "    reduce_first_axis_ = true;\n",
    "  } else {\n",
    "    // Starting from the (dim_index)-th dimension, dimensions\n",
    "    // alternates between runs that need to be reduced and runs that\n",
    "    // don't.\n",
    "    //\n",
    "    // NOTE: If a dimension has size 1, we group it as the current\n",
    "    // run so that we can minimize the number of runs.\n",
    "    //\n",
    "    // E.g., when we want to reduce a tensor of shape [2, 1, 3, 1,\n",
    "    // 5] by axes = [1, 4], we should treat the tensor as a [6, 5]\n",
    "    // and reduce by axes = [1] (i.e., the output is shape [6]).\n",
    "    reduce_first_axis_ = bitmap[dim_index];\n",
    "    data_reshape_.push_back(data.dim_size(dim_index));\n",
    "    ++dim_index;\n",
    "    for (; dim_index < data.dims(); ++dim_index) {\n",
    "      const auto size = data.dim_size(dim_index);\n",
    "      if (size == 1) {\n",
    "        bitmap[dim_index] = bitmap[dim_index - 1];\n",
    "      }\n",
    "      if (bitmap[dim_index - 1] != bitmap[dim_index]) {\n",
    "        // Starts a new run of reduce or !reduce.\n",
    "        data_reshape_.push_back(size);\n",
    "      } else {\n",
    "        // Continue a run of reduce or !reduce.\n",
    "        data_reshape_.back() *= size;\n",
    "      }\n",
    "    }\n",
    "    // If reduce_first_axis_ is true (input's dimension 0, 2, 4, etc\n",
    "    // are reduced), data_reshape_[1, 3, 5, ...]  is out_reshape_,\n",
    "    // otherwise, data_reshape_[0, 2, 4, ...] is.\n",
    "    for (size_t i = reduce_first_axis_ ? 1 : 0; i < data_reshape_.size();\n",
    "         i += 2) {\n",
    "      out_reshape_.push_back(data_reshape_[i]);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  VLOG(1) << \"data reshape: \" << str_util::Join(data_reshape_, \",\");\n",
    "  VLOG(1) << \"out  reshape: \" << str_util::Join(out_reshape_, \",\");\n",
    "  VLOG(1) << \"out    shape: \" << str_util::Join(out_shape_, \",\");\n",
    "  return Status::OK();\n",
    "}\n",
    "\n",
    "}  // namespace tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "#ifndef TENSORFLOW_KERNELS_REDUCTION_OPS_COMMON_H_\n",
    "#define TENSORFLOW_KERNELS_REDUCTION_OPS_COMMON_H_\n",
    "\n",
    "#define EIGEN_USE_THREADS\n",
    "\n",
    "#include \"third_party/eigen3/Eigen/Core\"\n",
    "#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n",
    "\n",
    "#include \"tensorflow/core/framework/numeric_op.h\"\n",
    "#include \"tensorflow/core/framework/op_kernel.h\"\n",
    "#include \"tensorflow/core/framework/register_types.h\"\n",
    "#include \"tensorflow/core/framework/tensor.h\"\n",
    "#include \"tensorflow/core/framework/types.h\"\n",
    "#include \"tensorflow/core/kernels/reduction_ops.h\"\n",
    "#include \"tensorflow/core/kernels/transpose_functor.h\"\n",
    "#include \"tensorflow/core/lib/core/status.h\"\n",
    "#include \"tensorflow/core/lib/gtl/inlined_vector.h\"\n",
    "#include \"tensorflow/core/platform/logging.h\"\n",
    "\n",
    "namespace tensorflow {\n",
    "\n",
    "typedef Eigen::ThreadPoolDevice CPUDevice;\n",
    "typedef Eigen::GpuDevice GPUDevice;\n",
    "#ifdef TENSORFLOW_USE_SYCL\n",
    "typedef Eigen::SyclDevice SYCLDevice;\n",
    "#endif  // TENSORFLOW_USE_SYCL\n",
    "\n",
    "template <typename Device>\n",
    "struct Constants {\n",
    "  // Derive Index type. int (32-bit) or long (64-bit) depending on the\n",
    "  // compile-time configuration. \"float\" here is not relevant.\n",
    "  // TODO(zhifengc): Moves the definition to TTypes.\n",
    "  typedef TTypes<float>::Tensor::Index Index;\n",
    "  Eigen::array<Index, 1> kZero;\n",
    "  Eigen::array<Index, 1> kOne;\n",
    "  Eigen::array<Index, 2> kZeroTwo;\n",
    "\n",
    "  Constants() {\n",
    "    kZero[0] = 0;\n",
    "    kOne[0] = 1;\n",
    "    kZeroTwo[0] = 0;\n",
    "    kZeroTwo[1] = 2;\n",
    "  }\n",
    "};\n",
    "\n",
    "#if defined(EIGEN_HAS_INDEX_LIST)\n",
    "struct ConstantsBase {\n",
    "  const Eigen::IndexList<Eigen::type2index<0>> kZero;\n",
    "  const Eigen::IndexList<Eigen::type2index<1>> kOne;\n",
    "  const Eigen::IndexList<Eigen::type2index<0>, Eigen::type2index<2>> kZeroTwo;\n",
    "};\n",
    "template <>\n",
    "struct Constants<CPUDevice> : ConstantsBase {};\n",
    "#ifdef TENSORFLOW_USE_SYCL\n",
    "template <>\n",
    "struct Constants<SYCLDevice> : ConstantsBase {};\n",
    "#endif  // TENSORFLOW_USE_SYCL\n",
    "#endif  // EIGEN_HAS_INDEX_LIST\n",
    "\n",
    "class ReductionHelper {\n",
    " public:\n",
    "  ReductionHelper() : reduce_first_axis_(false) {}\n",
    "\n",
    "  Status Simplify(const Tensor& data, const Tensor& axis, const bool keep_dims);\n",
    "\n",
    "  // We need to do roughly:\n",
    "  //   tmp_out = allocate(out_reshape())\n",
    "  //   tmp_out.reshape(out_reshape) = data.reshape(data_reshape).reduce(axes)\n",
    "  //   out = tmp_out.reshape(out_shape)\n",
    "\n",
    "  // The reduction result must be allocated with this shape.\n",
    "  TensorShape out_reshape() const;\n",
    "\n",
    "  // The final output shape must be allocated with this shape.\n",
    "  TensorShape out_shape() const;\n",
    "\n",
    "  // The reduction is on a reshaped tensor of this rank.\n",
    "  int ndims() const { return data_reshape_.size(); }\n",
    "\n",
    "  // True if need to reduce the 0-th dimension.\n",
    "  bool reduce_first_axis() const { return reduce_first_axis_; }\n",
    "\n",
    "  // The output is reshaped.\n",
    "  template <typename T, int N>\n",
    "  typename TTypes<T, N>::Tensor out(Tensor* out) {\n",
    "    return out->shaped<T, N>(out_reshape_);\n",
    "  }\n",
    "\n",
    "  // The input is reshaped.\n",
    "  template <typename T, int N>\n",
    "  typename TTypes<T, N>::ConstTensor in(const Tensor& data) {\n",
    "    return data.shaped<T, N>(data_reshape_);\n",
    "  }\n",
    "\n",
    "  // Shape of shuffled input\n",
    "  TensorShape data_reshape() const {\n",
    "    TensorShape shape;\n",
    "    for (auto s : data_reshape_) shape.AddDim(s);\n",
    "    return shape;\n",
    "  }\n",
    "\n",
    "  // Shape with all reduction dimensions at the end\n",
    "  TensorShape shuffled_shape();\n",
    "\n",
    "  // Permutation of reduced dims needed to put reduction dimensions at the end\n",
    "  gtl::InlinedVector<int32, 8> permutation();\n",
    "\n",
    " private:\n",
    "  bool reduce_first_axis_;  // True if need to reduce the 0-th dimension.\n",
    "  gtl::InlinedVector<int64, 4> data_reshape_;  // Reshape data before reduction.\n",
    "  gtl::InlinedVector<int64, 4> out_shape_;     // The final output shape.\n",
    "  gtl::InlinedVector<int64, 4> out_reshape_;   // Reshape output for reduction.\n",
    "};\n",
    "\n",
    "// For operations where the output is a reduction function along some\n",
    "// dimensions of the input.\n",
    "template <typename Device, class T, typename Tperm, typename Reducer>\n",
    "class ReductionOp : public OpKernel {\n",
    " public:\n",
    "  explicit ReductionOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n",
    "    const DataType dt = DataTypeToEnum<T>::v();\n",
    "    const DataType pt = DataTypeToEnum<Tperm>::v();\n",
    "    OP_REQUIRES_OK(ctx, ctx->MatchSignature({dt, pt}, {dt}));\n",
    "\n",
    "    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"keep_dims\", &keep_dims_));\n",
    "  }\n",
    "\n",
    "  void Compute(OpKernelContext* ctx) override {\n",
    "    const Tensor& data = ctx->input(0);\n",
    "    const Tensor& axes = ctx->input(1);\n",
    "    VLOG(1) << \"data shape: \" << data.shape().DebugString();\n",
    "    VLOG(1) << \"axes      : \" << axes.SummarizeValue(10);\n",
    "\n",
    "    ReductionHelper helper;\n",
    "    OP_REQUIRES_OK(ctx, helper.Simplify(data, axes, keep_dims_));\n",
    "    CHECK_GE(helper.ndims(), 0);\n",
    "\n",
    "    if (helper.ndims() == 0 ||\n",
    "        (helper.ndims() == 1 && !helper.reduce_first_axis())) {\n",
    "      // Special case. Reduces nothing.  It is unclear why this is\n",
    "      // necessary, but tests fail without it.  Look into why this\n",
    "      // case occurs.\n",
    "      Tensor out;\n",
    "      if (!out.CopyFrom(data, helper.out_shape())) {\n",
    "        ctx->SetStatus(errors::Internal(\"Error during reduction copy.\"));\n",
    "      }\n",
    "      ctx->set_output(0, out);\n",
    "      return;\n",
    "    }\n",
    "\n",
    "    // We must allocate temp tensors using the same alloc attr as\n",
    "    // output(0) because it is returned as output(0) in the end.\n",
    "    const AllocatorAttributes alloc_attr = ctx->output_alloc_attr(0);\n",
    "\n",
    "    // A temporary tensor whose size matches the size of the reduced\n",
    "    // output.\n",
    "    Tensor tmp_out;\n",
    "    OP_REQUIRES_OK(\n",
    "        ctx, ctx->allocate_temp(ctx->expected_output_dtype(0),\n",
    "                                helper.out_reshape(), &tmp_out, alloc_attr));\n",
    "\n",
    "    typedef functor::ReduceFunctor<Device, Reducer> Functor;\n",
    "    Constants<Device> constants;\n",
    "    const Device& d = ctx->eigen_device<Device>();\n",
    "    Reducer reducer;\n",
    "\n",
    "    if (tmp_out.NumElements() == 0) {\n",
    "      // Nothing to do, fall through to final reshaping.\n",
    "    } else if (data.NumElements() == 0) {\n",
    "      // Degenerate reduction where the input is empty but the output is\n",
    "      // nonempty (thus tmp_out.NumElements() > 0), and we must fill the output\n",
    "      // with identity elements.  Example: tf.reduce_sum(tf.zeros((0, 3)), [0]).\n",
    "      // Eigen sometimes crashes in this case, so we do it manually.\n",
    "      Functor::FillIdentity(d, tmp_out.flat<T>(), reducer);\n",
    "    } else if ((helper.ndims() == 1) && helper.reduce_first_axis()) {\n",
    "      // Reduce to a scalar.\n",
    "      Functor::Reduce(ctx, helper.out<T, 0>(&tmp_out), helper.in<T, 1>(data),\n",
    "                      constants.kZero, reducer);\n",
    "    } else if ((helper.ndims() == 2) && helper.reduce_first_axis()) {\n",
    "      // Can be viewed as a reduction of a matrix along 1st dimension.\n",
    "      Functor::Reduce(ctx, helper.out<T, 1>(&tmp_out), helper.in<T, 2>(data),\n",
    "                      constants.kZero, reducer);\n",
    "    } else if ((helper.ndims() == 2) && !helper.reduce_first_axis()) {\n",
    "      // Can be viewed as a reduction of a matrix along 2nd dimension.\n",
    "      Functor::Reduce(ctx, helper.out<T, 1>(&tmp_out), helper.in<T, 2>(data),\n",
    "                      constants.kOne, reducer);\n",
    "    } else if ((helper.ndims() == 3) && helper.reduce_first_axis()) {\n",
    "      // Can be viewed as a reduction of a 3D tensor along 1st and 3rd\n",
    "      // dimensions.\n",
    "      Functor::Reduce(ctx, helper.out<T, 1>(&tmp_out), helper.in<T, 3>(data),\n",
    "                      constants.kZeroTwo, reducer);\n",
    "    } else if ((helper.ndims() == 3) && !helper.reduce_first_axis()) {\n",
    "      // Can be viewed as a reduction of a 3D tensor along 2nd dimension.\n",
    "      Functor::Reduce(ctx, helper.out<T, 2>(&tmp_out), helper.in<T, 3>(data),\n",
    "                      constants.kOne, reducer);\n",
    "    } else {\n",
    "      // If we don't hit one of the cases above, transpose the data so that\n",
    "      // all reduced dimensions are last and reuse the 2-D -> 1-D case.\n",
    "      Tensor data_reshaped;\n",
    "      CHECK(data_reshaped.CopyFrom(data, helper.data_reshape()));\n",
    "      Tensor shuffled;\n",
    "      OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::value,\n",
    "                                             helper.shuffled_shape(), &shuffled,\n",
    "                                             alloc_attr));\n",
    "      OP_REQUIRES_OK(\n",
    "          ctx, DoTranspose(d, data_reshaped, helper.permutation(), &shuffled));\n",
    "      const int64 unreduced = tmp_out.NumElements();\n",
    "      const int64 reduced = shuffled.NumElements() / unreduced;\n",
    "      const Tensor& const_shuffled = shuffled;\n",
    "      Functor::Reduce(ctx, tmp_out.flat<T>(),\n",
    "                      const_shuffled.shaped<T, 2>({unreduced, reduced}),\n",
    "                      constants.kOne, reducer);\n",
    "    }\n",
    "\n",
    "    // Set the real output using the contents of the reduction but the\n",
    "    // real expected output shape.  The number of elements should\n",
    "    // match between the two shapes.\n",
    "    Tensor out;\n",
    "    if (!out.CopyFrom(tmp_out, helper.out_shape())) {\n",
    "      ctx->SetStatus(errors::Internal(\"Error during reduction copy.\"));\n",
    "    }\n",
    "    if (ctx->track_allocations()) {\n",
    "      ctx->record_temp_memory_size(-static_cast<int64>(out.AllocatedBytes()));\n",
    "    }\n",
    "    ctx->set_output(0, out);\n",
    "  }\n",
    "\n",
    " private:\n",
    "  // True if the number of dimensions should be maintained.\n",
    "  bool keep_dims_;\n",
    "};\n",
    "\n",
    "namespace functor {\n",
    "\n",
    "template <typename Device, typename Reducer>\n",
    "struct ReduceFunctorBase {\n",
    "  template <typename OUT_T, typename IN_T, typename ReductionAxes>\n",
    "  static void Reduce(OpKernelContext* ctx, OUT_T out, IN_T in,\n",
    "                     const ReductionAxes& reduction_axes,\n",
    "                     const Reducer& reducer) {\n",
    "    const Device& d = ctx->eigen_device<Device>();\n",
    "    ReduceEigenImpl(d, out, in, reduction_axes, reducer);\n",
    "  }\n",
    "\n",
    "  template <typename OUT_T>\n",
    "  static void FillIdentity(const Device& d, OUT_T out, const Reducer& reducer) {\n",
    "    FillIdentityEigenImpl(d, out, reducer);\n",
    "  }\n",
    "};\n",
    "\n",
    "template <typename Reducer>\n",
    "struct ReduceFunctor<CPUDevice, Reducer>\n",
    "    : ReduceFunctorBase<CPUDevice, Reducer> {};\n",
    "#if TENSORFLOW_USE_SYCL\n",
    "template <typename Reducer>\n",
    "struct ReduceFunctor<SYCLDevice, Reducer>\n",
    "    : ReduceFunctorBase<SYCLDevice, Reducer> {};\n",
    "#endif  // TENSORFLOW_USE_SYCL\n",
    "\n",
    "}  // namespace functor\n",
    "}  // namespace tensorflow\n",
    "\n",
    "#endif  // TENSORFLOW_KERNELS_REDUCTION_OPS_COMMON_H_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow/tensorflow/core/kernels/reduction_ops_gpu_float.cu.cc\n",
    "```cpp\n",
    "#if GOOGLE_CUDA\n",
    "\n",
    "#define EIGEN_USE_GPU\n",
    "\n",
    "#include \"tensorflow/core/kernels/reduction_gpu_kernels.cu.h\"\n",
    "\n",
    "namespace tensorflow {\n",
    "namespace functor {\n",
    "\n",
    "typedef Eigen::GpuDevice GPUDevice;\n",
    "\n",
    "// Derive Index type. int (32-bit) or long (64-bit) depending on the\n",
    "// compile-time configuration. \"float\" here is not relevant.\n",
    "// TODO(zhifengc): Moves the definition to TTypes.\n",
    "typedef TTypes<float>::Tensor::Index Index;\n",
    "\n",
    "// T: the data type\n",
    "// REDUCER: the reducer functor\n",
    "// NUM_AXES: the number of axes to reduce\n",
    "// IN_DIMS: the number of dimensions of the input tensor\n",
    "#define DEFINE(T, REDUCER, IN_DIMS, NUM_AXES)                          \\\n",
    "  template void ReduceFunctor<GPUDevice, REDUCER>::Reduce(             \\\n",
    "      OpKernelContext* ctx, TTypes<T, IN_DIMS - NUM_AXES>::Tensor out, \\\n",
    "      TTypes<T, IN_DIMS>::ConstTensor in,                              \\\n",
    "      const Eigen::array<Index, NUM_AXES>& reduction_axes,             \\\n",
    "      const REDUCER& reducer);\n",
    "\n",
    "#define DEFINE_IDENTITY(T, REDUCER)                              \\\n",
    "  template void ReduceFunctor<GPUDevice, REDUCER>::FillIdentity( \\\n",
    "      const GPUDevice& d, TTypes<T>::Vec out, const REDUCER& reducer);\n",
    "\n",
    "#define DEFINE_FOR_TYPE_AND_R(T, R) \\\n",
    "  DEFINE(T, R, 1, 1);               \\\n",
    "  DEFINE(T, R, 2, 1);               \\\n",
    "  DEFINE(T, R, 3, 1);               \\\n",
    "  DEFINE(T, R, 3, 2);               \\\n",
    "  DEFINE_IDENTITY(T, R)\n",
    "\n",
    "#define DEFINE_FOR_ALL_REDUCERS(T)                           \\\n",
    "  DEFINE_FOR_TYPE_AND_R(T, Eigen::internal::SumReducer<T>);  \\\n",
    "  DEFINE_FOR_TYPE_AND_R(T, Eigen::internal::MeanReducer<T>); \\\n",
    "  DEFINE_FOR_TYPE_AND_R(T, Eigen::internal::MinReducer<T>);  \\\n",
    "  DEFINE_FOR_TYPE_AND_R(T, Eigen::internal::MaxReducer<T>);  \\\n",
    "  DEFINE_FOR_TYPE_AND_R(T, Eigen::internal::ProdReducer<T>)\n",
    "\n",
    "DEFINE_FOR_ALL_REDUCERS(float);\n",
    "#undef DEFINE_FOR_ALL_REDUCERS\n",
    "#undef DEFINE_FOR_TYPE_AND_R\n",
    "#undef DEFINE\n",
    "\n",
    "}  // end namespace functor\n",
    "}  // end namespace tensorflow\n",
    "\n",
    "#endif  // GOOGLE_CUDA\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:libspn]",
   "language": "python",
   "name": "conda-env-libspn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
