{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import libspn as spn\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of values of each variable\n",
    "num_vals = 10 # 20  \n",
    "\n",
    "# Number of samples in dataset\n",
    "num_samples = 1000\n",
    "\n",
    "# Batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Num epochs\n",
    "num_epochs = 30\n",
    "\n",
    "batches_per_epoch = num_samples/batch_size\n",
    "print(batches_per_epoch)\n",
    "if int(batches_per_epoch) != batches_per_epoch:\n",
    "    raise Exception\n",
    "batches_per_epoch=int(batches_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Type of input distributions for dense generator\n",
    "input_dist = spn.DenseSPNGenerator.InputDist.MIXTURE\n",
    "\n",
    "# Number of input mixtures for dense generator\n",
    "num_input_mixtures = 10 # 10?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Additive smoothing during learning\n",
    "additive_smoothing=0\n",
    "min_additive_smoothing=0\n",
    "smoothing_decay=0.2\n",
    "initial_accum_value = 100\n",
    "\n",
    "# Weight initialization\n",
    "# weight_init_value = 1\n",
    "weight_init_value = tf.initializers.random_uniform(0, 1)\n",
    "\n",
    "# Type of inference during upward pass of learning\n",
    "value_inference_type = spn.InferenceType.MARGINAL\n",
    "\n",
    "# Add random values before max\n",
    "add_random=None\n",
    "\n",
    "use_unweighted=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "components = [spn.GaussianMixtureDataset.Component(0.1, [1,1], [[1,0],[0,1]]),\n",
    "              spn.GaussianMixtureDataset.Component(0.2, [1,4], [[1,0.4],[0.8,1]]),\n",
    "              spn.GaussianMixtureDataset.Component(0.1, [1,8], [[1,0],[0,1]]),\n",
    "              spn.GaussianMixtureDataset.Component(0.1, [3,4], [[0.5,0],[0,0.7]]),\n",
    "              spn.GaussianMixtureDataset.Component(0.1, [3,8], [[1,0],[0,1]]),\n",
    "              spn.GaussianMixtureDataset.Component(0.1, [5,4], [[1,0],[0,1]]),\n",
    "              spn.GaussianMixtureDataset.Component(0.3, [5,1], [[1,0.5],[0.5,1]])]\n",
    "\n",
    "train_set = spn.GaussianMixtureDataset(components=components,\n",
    "                                     num_samples=num_samples,\n",
    "                                     num_epochs=num_epochs,\n",
    "                                     batch_size=batch_size,\n",
    "                                     shuffle=True,\n",
    "                                     num_threads=1,\n",
    "                                     allow_smaller_final_batch=False,\n",
    "                                     num_vals=num_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_samples, labels, true_likelihoods = train_set.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate SPN Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ivs = spn.IVs(num_vars=2, num_vals=num_vals)\n",
    "dense_gen = spn.DenseSPNGenerator(num_decomps=1, num_subsets=2, num_mixtures=1, \n",
    "                                  input_dist=input_dist, \n",
    "                                  num_input_mixtures=num_input_mixtures)\n",
    "root = dense_gen.generate(ivs)\n",
    "spn.generate_weights(root, initializer=weight_init_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spn.display_spn_graph(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Learning Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ivs.attach_feed(train_samples)\n",
    "additive_smoothing_var = tf.Variable(additive_smoothing, dtype=spn.conf.dtype)\n",
    "learning = spn.EMLearning(root, log=True, value_inference_type = value_inference_type,\n",
    "                          additive_smoothing=additive_smoothing_var, add_random=add_random,\n",
    "                         initial_accum_value=initial_accum_value, use_unweighted=use_unweighted)\n",
    "init_weights = spn.initialize_weights(root)\n",
    "reset_accumulators = learning.reset_accumulators()\n",
    "accumulate_updates = learning.accumulate_updates()\n",
    "update_spn = learning.update_spn()\n",
    "train_likelihood=learning.value.values[root]\n",
    "avg_train_likelihood = tf.reduce_mean(train_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spn.display_tf_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_set = spn.IntGridDataset(num_dims=2,\n",
    "                              num_vals=num_vals,\n",
    "                              num_epochs=1,\n",
    "                              batch_size=1000,\n",
    "                              shuffle=False,\n",
    "                              num_threads=1,\n",
    "                              allow_smaller_final_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_samples = test_set.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Testing Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ivs.attach_feed(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_likelihoods = root.get_log_value(inference_type=spn.InferenceType.MARGINAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spn.display_tf_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with spn.session() as (sess, run):\n",
    "    sess.run(additive_smoothing_var.initializer)\n",
    "    sess.run(init_weights)\n",
    "    sess.run(reset_accumulators)\n",
    "      \n",
    "    # Run training\n",
    "    epoch=1\n",
    "    epoch_done=0\n",
    "    epoch_batch=0\n",
    "    try:\n",
    "        while run():\n",
    "            # Calculcate epoch and batch\n",
    "            epoch_batch+=1\n",
    "            if epoch_batch>batches_per_epoch:\n",
    "                epoch_batch=1\n",
    "                epoch+=1\n",
    "            print(\"Epoch: %s Batch: %s\" % (epoch, epoch_batch))\n",
    "            # Every epoch, including first\n",
    "            if epoch_done != epoch:\n",
    "                epoch_done = epoch\n",
    "                # Save SPN\n",
    "                saver=spn.JSONSaver(\"/home/czikus/model_%s.spn\" % (epoch-1), pretty=True)\n",
    "                saver.save(root)\n",
    "                # Reset/init accumulators\n",
    "                # sess.run(reset_accumulators)\n",
    "                # Adjust additive smoothing\n",
    "                ads=max(np.exp(-(epoch-1)*smoothing_decay)*additive_smoothing, min_additive_smoothing)\n",
    "                sess.run(additive_smoothing_var.assign(ads))\n",
    "                print(\"Smoothing: \", sess.run(additive_smoothing_var))\n",
    "                print(\"Accumulators RESET!\")\n",
    "            # Run accumulate_updates\n",
    "            train_samples_arr, labels_arr, train_likelihoods_arr, avg_train_likelihood_val, \\\n",
    "            true_likelihoods_arr, _, \\\n",
    "            root_accum_arr, root_weights = \\\n",
    "                sess.run([train_samples, labels, \n",
    "                          train_likelihood, avg_train_likelihood,\n",
    "                          true_likelihoods,\n",
    "                          accumulate_updates,\n",
    "                          # Testing\n",
    "                          learning.root_accum(), root.weights.node.variable\n",
    "                         ])\n",
    "            # Print avg likelihood of this batch data on previous batch weights\n",
    "            print(\"Avg likelihood (this batch data on previous weights): %s\" % (avg_train_likelihood_val))\n",
    "\n",
    "            # ----------------------------\n",
    "            # TEST STUFF\n",
    "            # - accumulator for root\n",
    "            # print(\"This batch data:\", train_samples_arr)\n",
    "            print(\"This batch accum:\", root_accum_arr)\n",
    "            print(\"Preious batch weights:\", root_weights)\n",
    "            \n",
    "            # ----------------------------\n",
    "\n",
    "            # Update weights\n",
    "            sess.run(update_spn)\n",
    "            \n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"TRAINING DONE!\")\n",
    "    \n",
    "    # Run testing\n",
    "    try:\n",
    "        while run():\n",
    "            test_samples_arr, test_likelihoods_arr = sess.run([test_samples, test_likelihoods])\n",
    "#            spn.plot_2d(test_samples_arr[:, 0], test_samples_arr[:, 1], probs=likelihoods_arr.ravel(), jitter=False)\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print(\"TESTING DONE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spn.plot_2d(train_samples_arr[:, 0], train_samples_arr[:, 1], labels=labels_arr[:, 0], probs=true_likelihoods_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# spn.plot_2d(train_samples_arr[:, 0], train_samples_arr[:, 1], labels=labels_arr[:, 0],\n",
    "#             probs=(np.exp(train_likelihoods_arr.ravel())), jitter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spn.plot_2d(test_samples_arr[:, 0], test_samples_arr[:, 1], \n",
    "            probs=(np.exp(test_likelihoods_arr.ravel())), jitter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for s, l in zip(test_samples_arr, likelihoods_arr):\n",
    "#    print(s.tolist(), l.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_vars=tf.local_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_vars[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local_vars[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
